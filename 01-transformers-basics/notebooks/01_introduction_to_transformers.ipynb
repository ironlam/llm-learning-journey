{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction aux Transformers avec Hugging Face\n",
    "\n",
    "Ce notebook constitue un premier pas dans l'apprentissage des modèles de Transformers avec la bibliothèque Hugging Face. Il explore les concepts fondamentaux et présente quelques exemples d'utilisation pratique.\n",
    "\n",
    "## Objectifs\n",
    "- Comprendre l'architecture des modèles Transformers\n",
    "- Explorer la bibliothèque Hugging Face\n",
    "- Apprendre à utiliser des modèles pré-entraînés pour des tâches simples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Installation des packages nécessaires\n",
    "!pip install transformers datasets evaluate torch tiktoken protobuf sentencepiece"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation des bibliothèques"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, pipeline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Les pipelines : une façon simple d'utiliser les modèles pré-entraînés\n",
    "\n",
    "Les pipelines sont la façon la plus simple d'utiliser les modèles pré-entraînés pour les tâches courantes de NLP."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exemple 1 : Analyse de sentiment\n",
    "# Utilisons un modèle spécifique pour le français\n",
    "sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# Testons avec quelques phrases\n",
    "texts = [\n",
    "    \"J'ai adoré cet article, il est très bien écrit et informatif.\",\n",
    "    \"Ce contenu est décevant et ne répond pas à mes attentes.\",\n",
    "    \"L'article est intéressant mais manque de profondeur sur certains aspects.\"\n",
    "]\n",
    "\n",
    "for text in texts:\n",
    "    result = sentiment_analyzer(text)[0]\n",
    "    print(f\"Texte: {text}\")\n",
    "    print(f\"Sentiment: {result['label']}, Score: {result['score']:.4f}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exemple 2 : Génération de texte\n",
    "# Utilisons un modèle spécifique pour le français\n",
    "text_generator = pipeline(\"text-generation\", model=\"bigscience/bloom-560m\")\n",
    "\n",
    "prompt = \"Dans le monde des médias numériques, l'intelligence artificielle\"\n",
    "generated_text = text_generator(\n",
    "    prompt,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    truncation=True,\n",
    "    # Paramètres pour réduire les répétitions\n",
    "    no_repeat_ngram_size=2,\n",
    "    repetition_penalty=1.5,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(\"Texte généré:\")\n",
    "print(generated_text[0]['generated_text'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Exemple 3 : Question-Réponse avec un modèle multilingue\n",
    "# Utilisons un modèle plus simple qui fonctionne bien avec le français\n",
    "qa_pipeline = pipeline(\n",
    "    \"question-answering\",\n",
    "    model=\"deepset/xlm-roberta-base-squad2\"  # Modèle multilingue qui supporte le français\n",
    ")\n",
    "\n",
    "context = \"\"\"\n",
    "Le Figaro est un journal français fondé en 1826 sous le règne de Charles X. \n",
    "Il est à ce titre le plus ancien quotidien français encore publié. \n",
    "Il a été nommé d'après le personnage de Figaro, protagoniste du Barbier de Séville et \n",
    "du Mariage de Figaro de Beaumarchais. En 2022, Le Figaro est le premier quotidien national \n",
    "en termes de diffusion, avec 370 000 exemplaires diffusés par jour en moyenne.\n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Quand a été fondé Le Figaro ?\",\n",
    "    \"D'où vient le nom du journal ?\",\n",
    "    \"Quel est le tirage quotidien du Figaro en 2022 ?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    result = qa_pipeline(question=question, context=context)\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Réponse: {result['answer']}\")\n",
    "    print(f\"Score: {result['score']:.4f}\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Comprendre les tokenizers\n",
    "\n",
    "Les tokenizers sont essentiels dans le traitement du langage naturel. Ils convertissent le texte en tokens qui peuvent être traités par les modèles."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Chargement d'un tokenizer adapté au français\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Tokenization d'une phrase en français\n",
    "text = \"Comment les modèles de langage transforment-ils l'industrie des médias ?\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "print(\"Input IDs (représentation numérique des tokens):\")\n",
    "print(encoded_input[\"input_ids\"])\n",
    "\n",
    "print(\"\\nTokens décodés:\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(encoded_input[\"input_ids\"][0])\n",
    "print(tokens)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Explorer les embeddings des modèles\n",
    "\n",
    "Les embeddings sont des représentations vectorielles des mots ou phrases qui capturent leur signification sémantique."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Chargement d'un modèle adapté au français\n",
    "model = AutoModel.from_pretrained(\"camembert-base\")\n",
    "\n",
    "# Générer des embeddings pour quelques phrases en français\n",
    "sentences = [\n",
    "    \"Les articles de presse doivent être informatifs.\",\n",
    "    \"Le journalisme nécessite de l'objectivité.\",\n",
    "    \"Les médias numériques transforment l'industrie de la presse.\",\n",
    "    \"La technologie change notre façon de consommer l'information.\",\n",
    "    \"Les chats sont des animaux domestiques populaires.\"\n",
    "]\n",
    "\n",
    "# Fonction pour obtenir les embeddings d'une phrase\n",
    "def get_embedding(sentence):\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Utiliser l'embedding du token [CLS] comme représentation de la phrase\n",
    "    return outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Calculer les embeddings\n",
    "embeddings = [get_embedding(sentence) for sentence in sentences]\n",
    "embeddings = [embedding.flatten() for embedding in embeddings]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calculer la similarité cosinus entre les embeddings\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Afficher la matrice de similarité\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(similarity_matrix, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar()\n",
    "plt.title('Similarité cosinus entre les phrases')\n",
    "plt.xticks(range(len(sentences)), [f\"Phrase {i+1}\" for i in range(len(sentences))], rotation=45)\n",
    "plt.yticks(range(len(sentences)), [f\"Phrase {i+1}\" for i in range(len(sentences))])\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Afficher les phrases et leurs similarités\n",
    "for i, sentence1 in enumerate(sentences):\n",
    "    print(f\"Phrase {i+1}: {sentence1}\")\n",
    "    \n",
    "print(\"\\nMatrice de similarité:\")\n",
    "for i, sentence1 in enumerate(sentences):\n",
    "    for j, sentence2 in enumerate(sentences):\n",
    "        if i < j:  # Pour éviter les duplications\n",
    "            print(f\"Similarité entre Phrase {i+1} et Phrase {j+1}: {similarity_matrix[i][j]:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion et Réflexions\n",
    "\n",
    "Dans ce notebook, j'ai exploré les bases des Transformers avec Hugging Face, en me concentrant particulièrement sur l'utilisation de modèles adaptés au français. J'ai pu utiliser des pipelines pour diverses tâches de NLP, comprendre le fonctionnement des tokenizers et explorer les embeddings générés par ces modèles.\n",
    "\n",
    "### Ce que j'ai appris :\n",
    "- Les pipelines facilitent grandement l'utilisation des modèles pré-entraînés\n",
    "- Il est important de choisir des modèles adaptés à la langue traitée (français dans ce cas)\n",
    "- Les tokenizers jouent un rôle crucial dans la conversion du texte en données numériques\n",
    "- Les embeddings capturent la sémantique des phrases, permettant de mesurer leur similarité\n",
    "\n",
    "### Prochaines étapes :\n",
    "- Explorer la classification de texte avec les Transformers\n",
    "- Apprendre à fine-tuner des modèles pour des tâches spécifiques\n",
    "- Approfondir ma compréhension de l'architecture des Transformers\n",
    "- Explorer davantage les modèles spécifiques au français comme CamemBERT\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
