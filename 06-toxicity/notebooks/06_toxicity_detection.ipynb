{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-03T07:10:02.809484Z",
     "start_time": "2025-09-03T07:10:01.480447Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "\n",
    "# Chargement du pipeline de classification\n",
    "toxicity = pipeline(\"text-classification\",\n",
    "                    model=\"unitary/unbiased-toxic-roberta\",\n",
    "                    top_k=None,\n",
    "                    device=\"cpu\" )\n",
    "\n",
    "# Exemples de commentaires\n",
    "comments = [\n",
    "    \"Je suis très heureux de ce produit !\",\n",
    "    \"Tu es vraiment nul...\",\n",
    "    \"C’est une belle journée.\",\n",
    "    \"Ferme-la, personne ne veut t’écouter.\"\n",
    "]\n",
    "\n",
    "# Analyse\n",
    "results = [toxicity(c)[0] for c in comments]\n",
    "\n",
    "# Mise en DataFrame\n",
    "df = pd.DataFrame([\n",
    "    {\n",
    "        \"commentaire\": c,\n",
    "        **{r[\"label\"]: r[\"score\"] for r in res}\n",
    "    }\n",
    "    for c, res in zip(comments, results)\n",
    "])\n",
    "\n",
    "print(df)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             commentaire  toxicity    insult      male  \\\n",
      "0   Je suis très heureux de ce produit !  0.000561  0.000189  0.000150   \n",
      "1                  Tu es vraiment nul...  0.006597  0.002256  0.000180   \n",
      "2               C’est une belle journée.  0.000604  0.000190  0.000125   \n",
      "3  Ferme-la, personne ne veut t’écouter.  0.001127  0.000433  0.000184   \n",
      "\n",
      "   psychiatric_or_mental_illness    female    muslim  christian   obscene  \\\n",
      "0                       0.000094  0.000079  0.000073   0.000069  0.000053   \n",
      "1                       0.000211  0.000051  0.000066   0.000043  0.000836   \n",
      "2                       0.000086  0.000070  0.000062   0.000096  0.000054   \n",
      "3                       0.000092  0.000064  0.000055   0.000054  0.000118   \n",
      "\n",
      "      white    threat  identity_attack    jewish  homosexual_gay_or_lesbian  \\\n",
      "0  0.000042  0.000040         0.000030  0.000026                   0.000025   \n",
      "1  0.000074  0.000367         0.000091  0.000017                   0.000012   \n",
      "2  0.000054  0.000050         0.000028  0.000026                   0.000019   \n",
      "3  0.000051  0.000055         0.000033  0.000021                   0.000023   \n",
      "\n",
      "      black  sexual_explicit  severe_toxicity  \n",
      "0  0.000025         0.000023         0.000001  \n",
      "1  0.000030         0.000133         0.000003  \n",
      "2  0.000030         0.000021         0.000001  \n",
      "3  0.000022         0.000032         0.000001  \n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
